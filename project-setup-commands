------1)) SETUP DOCKERFILE-------

To make building of images more efficient - One RUN command
In that case we do not create so many layers on our system

0. By default we use argument DEV = false.
We override it by 'true' in Dockerfile to copy requirements.dev.txt
And install and run linting

ARG DEV=false

1) Create virtual env to store our dependencies
RUN python -m venv /py && \

2 Upgrade pyp package manager inside of our virtual environment
    /py/bin/pip install --upgrade pip && \
2.1 install postgresql client inside our alpine3 image
That will allow Psycopg2 package to connect to postgres
    apk add --update --no-cache postgresql-client && \
2.2 Creates a virtual dependencies group to delete after setup
    apk add --update --no-cache --virtual .tmp-build-deps
        build-base postgresql-dev musl-dev && \

3 Install our requirements inside docker image
    /py/bin/pip install -r /tmp/requirements.txt && \
3.1 Chec DEV variable and install dependencies for development env
    if [ $DEV = "true" ]; \
        then /py/bin/pip install -r requirements.dev.txt ; \
    fi && \
4 remove tmp folder with any extra dependencies to keep image light
    rm -rf /tmp && \
4.1 remove temporary dependecies (required for postgres setup)
    apk del .tmp-build-deps &&  \
5 add new user inside of docker image.
5.1 Do not use root user to restrict access
    adduser \
5.2 disable access by password
        --disabled-password \
5.3 Do not create home folder to keep docker image as lightweigh as possible
        --no-create-home \
5.4 Specify username
        django-user
6 Update PATH environment variable inside of image
ENV PATH="/py/bin:$PATH"

7 SWITCH USER TO OUR USER  so all next commands inside of docker container will run by our user
USER django-user


------- BUILD DOCKER IMAGE ---------
create app folder in project
docker build .


-------- Create docker-compose.yml file -------

Version of docker-compose syntax
version: '3.9'

services:
1. Name of our service
  app:
2. Specify that we would like to build our docker file inside of our current directory because of context: "."
    build:
      context: .
3. Specify port mapping. 8000 on our local machine to port 8000 inside of docker container
    ports:
      - "8000:8000"
4. Volumes are the way of mapping directories from our system to a docker container
That we do for bring changes of our code to a running container without rebuilding that
    volumes:
      - ./app:/app
5. Command that used to run our service
    command: >
      sh -c "python manage.py runserver 0.0.0.0:8000"

----------- Build  ---------
docker-compose build



------------ LINTING -------------
Create new file ".flake8" in "app" folder

docker-compose run --rm app sh -c "flake8"



----------- CREATE AND RUN DJANGO PROJECT -----------
docker-compose run --rm app sh -c "django-admin startproject app ."

docker-compose up

http://127.0.0.1:8000/



----------- CONFIGURE GITHUB ACTIONS -------------
Create config file
.github/workflows/checks.yml

set trigger
add steps running testing and linting

Configure DockerHub auth


------- WRITE TESTS FOLLOWING TDD -----------
SimpleTestCase
TestCase

docker-compose run --rm app sh -c "python manage.py test"


---------- CONFIGURING DATABASE -----------

DB Adapter package Psycopg2 to let django connect to db
Psycopg2-binary
- Ok for dev
- NOT OK for production
- It is a pre-packaged binary. Not optimized for prod environment os

Psycopg2
- Compiled from source for required environment.
- Requires additional dependencies to be Compiled
- Easy to install with Docker

Installing Psycopg2:
- C compiller
- python3-dev
- libpq-dev
Equivalent packages for alpine3
- postgresql-client
- build-base
- postgresql-dev
- musl-dev


Update seetings.py with db setup.
Following environ variables are set in docker-compose.yml in section app.environment:
# Database
# https://docs.djangoproject.com/en/3.2/ref/settings/#databases

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'HOST': os.environ.get('DB_HOST'),
        'NAME': os.environ.get('DB_NAME'),
        'USER': os.environ.get('DB_USER'),
        'PASSWORD': os.environ.get('DB_PASS'),
    }
}



----------- CREATE CORE APP -------------

----------- FIXING DB RACE CONDITION -----------

---------- CREATE USER MODEL ----------

Open settings.py and add to the bottom -> AUTH_USER_MODEL = 'core.User'

docker-compose run --rm app sh -c 'python manage.py makemigrations'

docker-compose run --rm app sh -c 'python manage.py wait_for_db && python manage.py migrate'
see next step for possible fixes
---------- FIX MIGRATION INCONSISTENCY ---------
docker volume ls
docker volume rm recipe-app-api_dev-db-data
docker-compose down
docker volume rm recipe-app-api_dev-db-data

docker-compose run --rm app sh -c 'python manage.py wait_for_db && python manage.py migrate'

---------- ADD SUPERUSER SUPPORT ------------

docker-compose run --rm app sh -c 'python manage.py createsuperuser'


---------- API DOC. DRF-SPECTACULAR ---------
requirements.txt add drf-spectacular>=0.15.1,<0.16
docker-compose build

settings.py add to INSTALLED_APPS:
    'rest_framework',
    'drf_spectacular',

settings.py add to the end.
REST_FRAMEWORK = {
    'DEFAULT_SCHEMA_CLASS': 'drf_spectacular.openapi.AutoSchema'
}


------------ CREATE USER API -----------
docker-compose run --rm app sh -c "python manage.py startapp user"

In new folder:
- remove admin and tests
- Create new folder 'tests' with file __init__.py

core -> settings.py:  add to INSTALLED_APPS new line 'user',

------------- CREATE RECIPE API -----------
Create test
Create model
Add model to admin.py
docker-compose run --rm app sh -c "python manage.py startapp makemigrations"
docker-compose run --rm app sh -c "python manage.py startapp recipe"
remove recipe/migrations, recipe/admin, recipe/tests, recipe/models
add folder recipe/tests
add file recipe/tests/__init__.py
core -> settings.py:  add to INSTALLED_APPS new line 'recipe',

----------- TAGS - Implement PATCH, DELETE ------------
views.py ->
class TagViewSet(
    mixins.DestroyModelMixin,
    mixins.UpdateModelMixin,


------------ INGREDIENTS ----------
Add to class Recipe(models.Model):
    - ingredients = models.ManyToManyField('Ingredient')

Add to admin.py -> admin.site.register(models.Ingredient)

------------ IMAGES API ----------
Pillow (Python Image Library)
>>>>>
Dockerfile -> add  "jpeg-dev" to line "apk add --update --no-cache postgresql-client"
"""
RUN python -m venv /py && \
    /py/bin/pip install --upgrade pip && \
    apk add --update --no-cache postgresql-client jpeg-dev && \
"""

Dockerfile -> add "zlib zlib-dev" to line "apk add --update --no-cache --virtual .tmp-build-deps \
        build-base postgresql-dev musl-dev zlib zlib-dev"

>>>>>>
requirements.txt -> add Pillow>=8.2.0,<8.3.0
docker-compose build
>>>>>>
Dockerfile -> add after user
mkdir -p /vol/web/media && \
    mkdir -p /vol/web/static && \
    chown -R django-user:django-user /vol && \
    chmod -R 755 /vol

docker-compose build


>>>>>
in docker-compose.yml
-> add volume to end ->  dev-static-data:
-> app:volumes -> add - dev-static-data:/vol/web

>>>>>
add to settings.py ->  # Static files (CSS, JavaScript, Images) ->
STATIC_URL = '/static/static/'
MEDIA_URL = '/static/media/'

MEDIA_ROOT = '/vol/web/media'
STATIC_ROOT = '/vol/web/static'


>>>>>
add to urls.py ->
from django.conf.urls.static import static
from django.conf import settings

AND

if setting.DEBUG:
    urlPatters += static(
        settings.MEDIA_URL,
        document_root=settings.MEDIA_ROOT,
    )


>>>>
To enable upload images through SWAGGER
add to settings.py  ->
SPECTACULAR_SETTINGS = {
    'COMPONENT_SPLIT_REQUEST': True,
}



----------- DEPLOYMENT -------------
Add to Dockerfile ->
COPY ./scripts /scripts

>
for UWSGI installation
add after "apk add --update --no-cache --virtual .tmp-build-deps \
        build-base postgresql-dev musl-dev zlib zlib-dev" ->
linux-headers

>
chmod -R +x /scripts
CMD ["run.sh"]

>
change to:
ENV PATH="/scripts:/py/bin:$PATH"
>>>>>>>>
add scripts/run.sh

>>>>>>>>
requirements add:
uwsgi>=2.0.19<2.1

>>>>>>>
add proxy/default.conf.tpl
add proxy/run.sh
add proxy/Dockerfile

>>>>>>>>

add docker-compose-deploy.yml

>>>>>>>
add .env.sample

>>>>>>>
in sttings.py ->
SECRET_KEY = os.environ.get('SECRET_KEY', 'changeme')
DEBUG = bool(int(os.environ.get('DEBUG', 0)))
ALLOWED_HOSTS.extend(
    filter(
        None,
        os.environ.get('ALLOWED_HOSTS', '').split(','),
    )
)

>>>>>>
add to docker-compose.yml -> app:environment:
- DEBUG=1



>>>>>>


-------- AWS EC2 Server setup ---------
connect to ec2 via ssh
ssh ec2-user@34.228.69.23
Add ssh to git
ssh-keygen -t ed25519 -b 4096
leave name and pass empty

cat ~/.ssh/id_ed25519.pub

sudo yum install git -y

sudo amazon-linux-extras install docker -y

sudo systemctl enable docker.service
sudo systemctl start docker.service
sudo usermod -aG docker ec2-user
exit
ssh ec2-user@34.228.69.23
sudo curl -L "https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

git clone

cp .env.sample .env
vi .env

to allowed hosts add -> Find in ec2 -> instance -> networking -> public DNS

>>>>
Find in ec2 -> instance -> networking -> public DNS
ec2-34-228-69-23.compute-1.amazonaws.com/admin

>>>>
docker-compose -f docker-compose-deploy.yml up -d

Create superuser
docker-compose -f docker-compose-deploy.yml run --rm app sh -c "pthon manage.py createsuperuser"
admin@gmail.com
Pwd1234

FETCH THE LOGS
docker-compose -f docker-compose-deploy.yml log

